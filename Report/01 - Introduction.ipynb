{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Toolbox - Assessment 02\n",
    "\n",
    "*This project is the 2nd assessment of the Data Science Toolbox module from the MSc Mathematics of Cybersecurity course at the University of Bristol (2020/21). There are 3 contributors to the project. While it is preferrable to be able to test models and draw solid conclusions, the most important parts of these assessments is to explore options available for good Data Science in the industry and learn new skills throughout the project.*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project we are tackling a data science problem on the MACCDC2012 conn.log (accessed from [Secrepo](http://www.secrepo.com/)) data set. The official brief from the assessment description was:\n",
    "\n",
    "\"Your challenge is to take a non-trivial model from your broader statistical experience and apply this to a classification or regression problem of your choice. The level of complexity required is such that there is likely to be an implementation in a package available that implements your chosen approach. You will then attempt to fit the model to this data in order to gain an insight into both the model, and the dataset.\"\n",
    "\n",
    "There were several challenges in this task and we had to address these in turn. Before even getting into anything interesting with the data, the first thing that we had to contend with was the initial data set. According to Secrepo the full compressed .gz file is 524MB; certainly a large file in itself, and to access the unencrypted .log file we would be looking at an even larger dataset. This is not practical for two main reasons:\n",
    "\n",
    "* Such a large dataset would greatly fill many devices RAM in and of itself - this is ignorning additional variables that we may define throughout our task that take up memory themself. Certainly, even the best of the PCs in our group struggled with handling this data without crashing the R kernel.\n",
    "\n",
    "* Even if we had a suitable pool of memory to store this data, not only does it take around 10 minutes to import the data itself - but any analysis over the variables, or functions on the a dataframe would take an extremely long time to compute. This is a processing issue and is almost more concerning than the memory issue, due to it potentially wasting a lot of time.\n",
    "\n",
    "In order to combat these issues the first thing that we did was break the data up into smaller datasets, of which we exported a huge variety of for potential uses later. Most of this data was not used in this report but was generated in advance just to make sure we had a variety of datasets to experiment with. These were always split off of the main dataframe using the `set.seed()` command in order to make the data reproducable if others did not have access to the data in our GitHub Repository. We split off a variety of data, including randomly sampled both at 1% and 5% intervals, and \"head\" data of each of these; this took the first 1 or 5 percent of the data in the dataset, preserving time order. More details of this process are found in the following document, [02 - Data import](https://github.com/Galeforse/DST-Assessment-02/blob/main/Report/02%20-%20Data%20import.ipynb).\n",
    "\n",
    "In [03 - Data Analysis](https://github.com/Galeforse/DST-Assessment-02/blob/main/Report/02%20-%20Data%20analysis.ipynb) We take a look at some of our outputted data from the previous file to get an idea of what we are dealing with. The most notable thing about this dataset is the large amount of missing data present. Due to our knowledge of imputation from lectures, after some exploratory data analysis we thought that it would be good to try to see if we could impute the missing data, using a simple method such as mean imputation as a baseline. This whole process provided a whole host of errors for us that we will reflect upon in the conclusion. The main things we are aiming to be find is a measure for how accurate each different imputation method is and if they are more accurate than the others that we tried. We tried several different R packages to help with our imputation that will be explained in each part of the report.\n",
    "\n",
    "After these first 2 documents we get onto some attempts with different methods at imputing missing data in the data set. Which will be explained more in depth in each individual document. And will be followed by a conclusion with our main findings from this project.\n",
    "\n",
    "The reader is asked to look at each item in the [Report](https://github.com/Galeforse/DST-Assessment-02/tree/main/Report) folder chronologically in order to follow a similar sequence of events as we conducted them to aid with overall clarity.\n",
    "\n",
    "## Contributors\n",
    "\n",
    "[Gabriel Grant](https://github.com/Galeforse/DST-Assessment-02/tree/main/Gabriel%20Grant)\n",
    "\n",
    "[Luke Hawley](https://github.com/Galeforse/DST-Assessment-02/tree/main/Luke%20Hawley)\n",
    "\n",
    "[Xiao Zhang](https://github.com/Galeforse/DST-Assessment-02/tree/main/Xiao%20Zhang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
